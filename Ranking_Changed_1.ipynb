{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "import networkx as nx\n",
    "import operator\n",
    "from cytoolz import itertoolz\n",
    "from spacy.tokens.span import Span as SpacySpan\n",
    "from spacy.tokens.token import Token as SpacyToken\n",
    "\n",
    "import compat\n",
    "import extract\n",
    "import vsm\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import numpy as np\n",
    "import spacy \n",
    "import textacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text used\n",
    "Citigroup analysts are predicting a full-on bear market within months based on historical trends, according to a new note by equity strategist Robert Buckland. Here's how you can protect your portfolio. Most everyone will suffer losses in a bear market short-sellers are winners, of course, but investors can decide now for themselves how much they are willing to risk says certified financial planner Alexander G. Koury of Values Quest, Inc. The first thing to do is check the current risk of the portfolio, Koury said. This will help the investor determine what would be the worst case scenario if the market were to go into a bear market. That means an investor will know how much they're willing to lose of their portfolio, and they can determine whether or not that is comfortable for them. Investors who don't plan to make withdrawals from their portfolios for decades could leave their investments be until the next bull market, but investors planning on retiring soon might want to limit their exposure. Koury recommends that investors should seek the help of either a financial planner or software to see if a reallocation is necessary to help them meet their goals. Set Aside What You Need To Live. In addition to limiting their exposure to equities, retirees and other investors living off of their portfolio's returns also should prioritize their living expenses over investing when the market's down. If you are taking income from your portfolio, always be sure you have a couple year's worth of withdrawals in money market or short term bonds, said Edward Snyder, certified financial planner at Oak Tree Advisors. The rest of your portfolio should be diversified among major asset classes, including intermediate term bonds. This should allow you to ride out a down market without having to sell stock investments while the market is down. Mentally Prepare Yourself. Your own bad investment decisions can cost your portfolio as much as market losses, certified financial planner Patrick Amey thinks. Prepare yourself emotionally to ride it out and tune out the noise, Amey said. Yes, your portfolio will go down in value. But you have the cash you need so you can give your portfolio the time it needs to recover. Stay consistent with you allocation and don't make knee jerk decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(u\"Citigroup analysts are predicting a full-on bear market within months based on historical trends, according to a new note by equity strategist Robert Buckland. Here's how you can protect your portfolio. Most everyone will suffer losses in a bear market short-sellers are winners, of course, but investors can decide now for themselves how much they are willing to risk says certified financial planner Alexander G. Koury of Values Quest, Inc. The first thing to do is check the current risk of the portfolio, Koury said. This will help the investor determine what would be the worst case scenario if the market were to go into a bear market. That means an investor will know how much they're willing to lose of their portfolio, and they can determine whether or not that is comfortable for them. Investors who don't plan to make withdrawals from their portfolios for decades could leave their investments be until the next bull market, but investors planning on retiring soon might want to limit their exposure. Koury recommends that investors should seek the help of either a financial planner or software to see if a reallocation is necessary to help them meet their goals. Set Aside What You Need To Live. In addition to limiting their exposure to equities, retirees and other investors living off of their portfolio's returns also should prioritize their living expenses over investing when the market's down. If you are taking income from your portfolio, always be sure you have a couple year's worth of withdrawals in money market or short term bonds, said Edward Snyder, certified financial planner at Oak Tree Advisors. The rest of your portfolio should be diversified among major asset classes, including intermediate term bonds. This should allow you to ride out a down market without having to sell stock investments while the market is down. Mentally Prepare Yourself. Your own bad investment decisions can cost your portfolio as much as market losses, certified financial planner Patrick Amey thinks. Prepare yourself emotionally to ride it out and tune out the noise, Amey said. Yes, your portfolio will go down in value. But you have the cash you need so you can give your portfolio the time it needs to recover. Stay consistent with you allocation and don't make knee jerk decision.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def key_terms_from_semantic_network(doc, normalize='lemma',\n",
    "                                    window_width=2, edge_weighting='cooc',\n",
    "                                    ranking_algo='pagerank', join_key_words=False,\n",
    "                                    n_keyterms=10, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract key terms from a document by ranking nodes in a semantic network of\n",
    "    terms, connected by edges and weights specified by parameters.\n",
    "    Args:\n",
    "        doc (``textacy.Doc`` or ``spacy.Doc``)\n",
    "        normalize (str or callable): if 'lemma', lemmatize terms; if 'lower',\n",
    "            lowercase terms; if None, use the form of terms as they appeared in\n",
    "            ``doc``; if a callable, must accept a ``spacy.Token`` and return a str,\n",
    "            e.g. :func:`textacy.spacier.utils.get_normalized_text()`\n",
    "        window_width (int): width of sliding window in which term\n",
    "            co-occurrences are said to occur\n",
    "        edge_weighting ('binary', 'cooc_freq'}): method used to\n",
    "            determine weights of edges between nodes in the semantic network;\n",
    "            if 'binary', edge weight is set to 1 for any two terms co-occurring\n",
    "            within `window_width` terms; if 'cooc_freq', edge weight is set to\n",
    "            the number of times that any two terms co-occur\n",
    "        ranking_algo ({'pagerank', 'divrank', 'bestcoverage'}):\n",
    "            algorithm with which to rank nodes in the semantic network;\n",
    "            `pagerank` is the canonical (and default) algorithm, but it prioritizes\n",
    "            node centrality at the expense of node diversity; the other two\n",
    "            attempt to balance centrality with diversity\n",
    "        join_key_words (bool): if True, join consecutive key words\n",
    "            together into longer key terms, taking the sum of the constituent words'\n",
    "            scores as the joined key term's combined score\n",
    "        n_keyterms (int or float): if int, number of top-ranked terms\n",
    "            to return as keyterms; if float, must be in the open interval (0, 1),\n",
    "            is converted to an integer by ``round(len(doc) * n_keyterms)``\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: sorted list of top ``n_keyterms`` key terms and\n",
    "        their corresponding ranking scores\n",
    "    Raises:\n",
    "        ValueError: if ``n_keyterms`` is a float but not in (0.0, 1.0]\n",
    "    \"\"\"\n",
    "    if isinstance(n_keyterms, float):\n",
    "        if not 0.0 < n_keyterms <= 1.0:\n",
    "            raise ValueError('`n_keyterms` must be an int, or a float between 0.0 and 1.0')\n",
    "        n_keyterms = int(round(len(doc) * n_keyterms))\n",
    "\n",
    "    include_pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "    if normalize == 'lemma':\n",
    "        word_list = [word.lemma_ for word in doc]\n",
    "        good_word_list = [word.lemma_ for word in doc\n",
    "                          if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "    elif normalize == 'lower':\n",
    "        word_list = [word.lower_ for word in doc]\n",
    "        good_word_list = [word.lower_ for word in doc\n",
    "                          if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "    elif not normalize:\n",
    "        word_list = [word.text for word in doc]\n",
    "        good_word_list = [word.text for word in doc\n",
    "                          if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "    else:\n",
    "        word_list = [normalize(word) for word in doc]\n",
    "        good_word_list = [normalize(word) for word in doc\n",
    "                          if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "\n",
    "    # HACK: omit empty strings, which happen as a bug in spacy as of v1.5\n",
    "    # and may well happen with ``normalize`` as a callable\n",
    "    # an empty string should never be considered a keyterm\n",
    "    good_word_list = [word for word in good_word_list if word]\n",
    "    \n",
    "    \n",
    "    ## the function is called as it is as it's in the same notebook\n",
    "    ## terms_to_semantic_network() is modified\n",
    "    ## TERMS_TO_SEMANTIC_NETWORK IS MODIFIED\n",
    "    graph = terms_to_semantic_network( good_word_list, window_width = window_width, edge_weighting = edge_weighting)\n",
    "\n",
    "    # rank nodes by algorithm, and sort in descending order\n",
    "   \n",
    "\n",
    "    if ranking_algo == 'pagerank':\n",
    "        word_ranks = nx.pagerank_scipy(graph, weight='weight')\n",
    "    elif ranking_algo == 'divrank':\n",
    "        word_ranks = rank_nodes_by_divrank(  graph, r=None, lambda_=kwargs.get('lambda_', 0.5), alpha=kwargs.get('alpha', 0.5))\n",
    "    elif ranking_algo == 'bestcoverage':\n",
    "          word_ranks = rank_nodes_by_bestcoverage(\n",
    "               graph, k=n_keyterms, c=kwargs.get('c', 1), alpha=kwargs.get('alpha', 1.0))\n",
    "\n",
    "    # bail out here if all we wanted was key *words* and not *terms*\n",
    "    \n",
    "    \n",
    "    if join_key_words is False:\n",
    "        return [(word, score) for word, score in\n",
    "                sorted(word_ranks.items(), key=operator.itemgetter(1), reverse=True)[:n_keyterms]]\n",
    "\n",
    "    \n",
    "    top_n = int(0.25 * len(word_ranks))\n",
    "    top_word_ranks = {word: rank for word, rank in\n",
    "                      sorted(word_ranks.items(), key=operator.itemgetter(1), reverse=True)[:top_n]}\n",
    "\n",
    "    # join consecutive key words into key terms\n",
    "    seen_joined_key_terms = set()\n",
    "    joined_key_terms = []\n",
    "    for key, group in itertools.groupby(word_list, lambda word: word in top_word_ranks):\n",
    "        if key is True:\n",
    "            words = list(group)\n",
    "            term = ' '.join(words)\n",
    "            if term in seen_joined_key_terms:\n",
    "                continue\n",
    "            seen_joined_key_terms.add(term)\n",
    "            joined_key_terms.append((term, sum(word_ranks[word] for word in words)))\n",
    "\n",
    "    return sorted(joined_key_terms, key=operator.itemgetter(1, 0), reverse=True)[:n_keyterms]\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keywords with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFUL\n"
     ]
    }
   ],
   "source": [
    "key_terms = key_terms_from_semantic_network( doc, normalize='lemma',\n",
    "                                    window_width=2, edge_weighting='embedding',\n",
    "                                    ranking_algo='pagerank', join_key_words=False,\n",
    "                                    n_keyterms=10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('portfolio', 0.07369445602364152),\n",
       " ('market', 0.0576838968632258),\n",
       " ('investor', 0.028771518488359696),\n",
       " ('case', 0.022916503859436804),\n",
       " ('value', 0.022668765270240723),\n",
       " ('equity', 0.022646900586409592),\n",
       " ('planner', 0.02264690058640958),\n",
       " ('term', 0.022341742486389593),\n",
       " ('investment', 0.020783941447813422),\n",
       " ('intermediate', 0.020511739216136447)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keywords with co occurence frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFUL\n"
     ]
    }
   ],
   "source": [
    "key_terms_new = key_terms_from_semantic_network( doc, normalize='lemma',\n",
    "                                    window_width=2, edge_weighting='cooc_freq',\n",
    "                                    ranking_algo='pagerank', join_key_words=False,\n",
    "                                    n_keyterms=10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('portfolio', 0.0667483389589195),\n",
       " ('market', 0.06015778159753895),\n",
       " ('investor', 0.04244749210697408),\n",
       " ('planner', 0.02634748492269935),\n",
       " ('financial', 0.024609262141485413),\n",
       " ('koury', 0.020062844208003718),\n",
       " ('investment', 0.019659376113500673),\n",
       " ('bear', 0.01867282743668402),\n",
       " ('equity', 0.016164106083899316),\n",
       " ('amey', 0.014749823019943726)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_terms_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for terms_to_semantic_network which makes the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper function 1 :  Returns emedding_matrix with a dictionary that stores the index of every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_embedding(terms):\n",
    "    filename = 'C:/Users/hp/Word_embeddings/GoogleNews-vectors-negative300.bin'\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format( filename , binary=True)\n",
    "\n",
    "    words_to_index = {}\n",
    "    i = 0;\n",
    "    for word in terms:\n",
    "        if not word in words_to_index:\n",
    "            words_to_index[str(word)] = i\n",
    "            i = i + 1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    embedding_matrix = np.zeros(( len(words_to_index) , 300 ))\n",
    "    for word , i in words_to_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "    return (embedding_matrix , words_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper function 2 :  Returns a nested dictiornary with the cosine similarity between the words that are within the window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "    to the definition of the dot product\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0 :\n",
    "        return 0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def get_cosine_mat(windows , embedding_matrix , words_to_index ):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from scipy import spatial\n",
    "    cosine_mat = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "  \n",
    "    for window in windows:\n",
    "        for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "            cosine_mat[w1][w2] = cos_sim(embedding_matrix[words_to_index[w1]] , embedding_matrix[words_to_index[w2]] )\n",
    "          \n",
    "            \n",
    "    return cosine_mat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper function 3 : Returns a nested dictiornary with the PMI between the words that are within the window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def count_of_single( Tokens , freqThreshold):          \n",
    "    word_freq = defaultdict(int)\n",
    "    fdist = defaultdict(int)\n",
    "    for a in Tokens:\n",
    "        fdist[a] += 1 \n",
    "    \n",
    "    for word , freq in sorted(fdist.items(), key=lambda k__v: (k__v[1],k__v[0])):\n",
    "        if freq > freqThreshold:\n",
    "            word_freq[word] = freq\n",
    "    return word_freq\n",
    "\n",
    "def count_of_bigrams( Tokens ,freqThreshold):\n",
    "        bigram_freq = defaultdict(int)\n",
    "\n",
    "        b = nltk.collocations.BigramCollocationFinder.from_words(Tokens)\n",
    "        b.apply_freq_filter(freqThreshold)\n",
    "       \n",
    "        for bigram, freq in b.ngram_fd.items():\n",
    "                bigram=\" \".join([bigram[0], bigram[1]])\n",
    "                bigram_freq[bigram] = freq\n",
    "        return bigram_freq\n",
    "    \n",
    "def pmi(w1, w2, unigram_freq , bigram_freq):\n",
    "\n",
    "    prob_word1 = unigram_freq[w1] / float(sum(unigram_freq.values()))\n",
    "    prob_word2 = unigram_freq[w2] / float(sum(unigram_freq.values()))\n",
    "    prob_word1_word2 = bigram_freq[\" \".join([w1, w2])] / float(sum(bigram_freq.values()))\n",
    "\n",
    "    if prob_word1_word2 == 0 :\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "\n",
    "        return math.log(prob_word1_word2/float(prob_word1*prob_word2),2)\n",
    "\n",
    "    except: # Occurs when calculating PMI for Out-of-Vocab words.\n",
    "\n",
    "        return 0\n",
    "\n",
    "def get_pmi_mat(terms , windows ):\n",
    "    pmi_mat = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "    unigram_freq = count_of_single( terms , 0)\n",
    "    bigram_freq = count_of_bigrams( terms , 0)\n",
    "   \n",
    "    for window in windows:\n",
    "        for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "            pmi_mat[w1][w2] = pmi( w1 , w2 ,  unigram_freq , bigram_freq )\n",
    "         \n",
    "            \n",
    "    return pmi_mat        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper function 4 :  Returns a nested dictionary with the score between all the words within the window size \n",
    "       score = pmi * similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_final_weights( cosine_mat , pmi_mat ):\n",
    "    \n",
    "    final_weights = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "    for (i , j) , (k , l) in zip(cosine_mat.items() ,pmi_mat.items()):\n",
    "        for (a , b) , (c , d) in zip(j.items() , l.items()):\n",
    "                final_weights[i][a] = b*d \n",
    "              \n",
    "          \n",
    "    return final_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified fucntion:\n",
    "     1. Option \" embedding \" is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terms_to_semantic_network(terms, normalize='lemma', window_width=10, edge_weighting='cooc_freq'):\n",
    "   \n",
    "    \"\"\"\n",
    "    Transform an ordered list of non-overlapping terms into a semantic network,\n",
    "    where each term is represented by a node with weighted edges linking it to\n",
    "    other terms that co-occur within ``window_width`` terms of itself.\n",
    "    Args:\n",
    "        terms (List[str] or List[``spacy.Token``])\n",
    "        normalize (str or Callable): If 'lemma', lemmatize terms; if 'lower',\n",
    "            lowercase terms; if false-y, use the form of terms as they appear\n",
    "            in ``terms``; if a callable, must accept a ``spacy.Token`` and return\n",
    "            a str, e.g. :func:`textacy.spacier.utils.get_normalized_text()`.\n",
    "            .. note:: This is applied to the elements of ``terms`` *only* if\n",
    "               it's a list of ``spacy.Token``.\n",
    "        window_width (int): Size of sliding window over ``terms`` that determines\n",
    "            which are said to co-occur. If 2, only immediately adjacent terms\n",
    "            have edges in the returned network.\n",
    "        edge_weighting ({'cooc_freq', 'binary'}): If 'cooc_freq', the nodes for\n",
    "            all co-occurring terms are connected by edges with weight equal to\n",
    "            the number of times they co-occurred within a sliding window;\n",
    "            if 'binary', all such edges have weight = 1.\n",
    "    Returns:\n",
    "        ``networkx.Graph``: Nodes in this network correspond to individual terms;\n",
    "        those that co-occur are connected by edges with weights determined\n",
    "        by ``edge_weighting``.\n",
    "    Notes:\n",
    "        - Be sure to filter out stopwords, punctuation, certain parts of speech, etc.\n",
    "          from the terms list before passing it to this function\n",
    "        - Multi-word terms, such as named entities and compound nouns, must be merged\n",
    "          into single strings or spacy.Tokens beforehand\n",
    "        - If terms are already strings, be sure to have normalized them so that\n",
    "          like terms are counted together; for example, by applying\n",
    "          :func:`textacy.spacier.utils.get_normalized_text()`\n",
    "    \"\"\"\n",
    "    if window_width < 2:\n",
    "        raise ValueError(\n",
    "            '`window_width` = {} is invalid; value must be >= 2'.format(window_width))\n",
    "    if not terms:\n",
    "        raise ValueError(\n",
    "            '`terms` = {} is invalid; it must contain at least 1 term '\n",
    "            'in the form of a string or spacy token'.format(terms))\n",
    "\n",
    "    # if len(terms) < window_width, cytoolz throws a StopIteration error\n",
    "    # which we don't want\n",
    "    if len(terms) < window_width:\n",
    "        LOGGER.info(\n",
    "            '`terms` has fewer items (%s) than the specified `window_width` (%s); '\n",
    "            'setting window width to %s',\n",
    "            len(terms), window_width, len(terms))\n",
    "        window_width = len(terms)\n",
    "\n",
    "    if isinstance(terms[0], compat.unicode_):\n",
    "        windows = itertoolz.sliding_window(window_width, terms)\n",
    "        to_return = list(windows)\n",
    "    elif isinstance(terms[0], SpacyToken):\n",
    "        if normalize == 'lemma':\n",
    "            windows = ((tok.lemma_ for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "        elif normalize == 'lower':\n",
    "            windows = ((tok.lower_ for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "        elif not normalize:\n",
    "            windows = ((tok.text for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "        else:\n",
    "            windows = ((normalize(tok) for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            'items in `terms` must be strings or spacy tokens, not {}'.format(type(terms[0])))\n",
    "\n",
    "  \n",
    "  \n",
    "    graph = nx.Graph()\n",
    "\n",
    "    if edge_weighting == 'cooc_freq':\n",
    "        cooc_mat = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "        for window in to_return:\n",
    "            for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "                cooc_mat[w1][w2] += 1\n",
    "               \n",
    "                               \n",
    "        graph.add_edges_from(\n",
    "            (w1, w2, {'weight': weight})\n",
    "            for w1, w2s in cooc_mat.items()\n",
    "            for w2, weight in w2s.items())\n",
    "    elif edge_weighting == 'binary':\n",
    "        graph.add_edges_from(\n",
    "            w1_w2 for window in windows\n",
    "            for w1_w2 in itertools.combinations(window, 2))\n",
    "    \n",
    "    ## option of embedding is added\n",
    "    elif edge_weighting == 'embedding':\n",
    "        embedding_matrix , word_to_index = get_word_embedding(terms)\n",
    "        cosine_mat = get_cosine_mat(to_return , embedding_matrix  , word_to_index )\n",
    "        pmi_mat = get_pmi_mat( terms , to_return )\n",
    "        final_weights = get_final_weights( cosine_mat , pmi_mat )\n",
    "        \n",
    "        for window in to_return:\n",
    "            for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "                graph.add_weighted_edges_from(\n",
    "                    (w1, w2, weight)\n",
    "                    for w1, w2s in final_weights.items()\n",
    "                    for w2, weight in w2s.items())\n",
    "   \n",
    "    print(\"SUCCESSFUL\")\n",
    "    return graph\n",
    "  \n",
    "   \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##return_graph = terms_to_semantic_network(terms, normalize='lemma', window_width=2, edge_weighting='embedding')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
