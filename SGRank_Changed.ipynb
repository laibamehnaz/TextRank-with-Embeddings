{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "import operator\n",
    "from decimal import Decimal\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from cytoolz import itertoolz\n",
    "\n",
    "import compat\n",
    "import extract\n",
    "import network\n",
    "#import similarity\n",
    "import vsm\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Used\n",
    "\n",
    "Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to \"learn\" (e.g., progressively improve performance on a specific task) with data, without being explicitly programmed.The name machine learning was coined in 1959 by Arthur Samuel.Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data – such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions, through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders, and computer vision.Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining, where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to \"produce reliable, repeatable decisions and results\" and uncover \"hidden insights\" through learning from historical relationships and trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to learn (e.g., progressively improve performance on a specific task) with data, without being explicitly programmed.The name machine learning was coined in 1959 by Arthur Samuel.Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data – such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions, through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders, and computer vision.Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining, where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to produce reliable, repeatable decisions and results and uncover hidden insights through learning from historical relationships and trends in the data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(u\"Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to learn (e.g., progressively improve performance on a specific task) with data, without being explicitly programmed.The name machine learning was coined in 1959 by Arthur Samuel.Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data – such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions, through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders, and computer vision.Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining, where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to produce reliable, repeatable decisions and results and uncover hidden insights through learning from historical relationships and trends in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## terms_to_semantic_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_embedding(terms):\n",
    "    filename = 'C:/Users/hp/Word_embeddings/GoogleNews-vectors-negative300.bin'\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format( filename , binary=True)\n",
    "\n",
    "    words_to_index = {}\n",
    "    i = 0;\n",
    "    for word in terms:\n",
    "        if not word in words_to_index:\n",
    "            words_to_index[str(word)] = i\n",
    "            i = i + 1\n",
    "        else:\n",
    "            continue\n",
    "   \n",
    "   \n",
    "    embedding_matrix = np.zeros(( len(words_to_index) , 300 ))\n",
    "    for word , i in words_to_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "    return (embedding_matrix , words_to_index)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "    to the definition of the dot product\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0 :\n",
    "        return 0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def get_cosine_mat(windows , embedding_matrix , words_to_index ):\n",
    "    from scipy import spatial\n",
    "    cosine_mat = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "  \n",
    "    for window in windows:\n",
    "        for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "               cosine_mat[w1][w2] = cos_sim(embedding_matrix[words_to_index[w1]] , embedding_matrix[words_to_index[w2]] )\n",
    "           \n",
    "            \n",
    "    return cosine_mat        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def count_of_single( Tokens , freqThreshold):          \n",
    "    word_freq = defaultdict(int)\n",
    "    fdist = defaultdict(int)\n",
    "    for a in Tokens:\n",
    "        fdist[a] += 1 \n",
    "    \n",
    "    for word , freq in sorted(fdist.items(), key=lambda k__v: (k__v[1],k__v[0])):\n",
    "        if freq > freqThreshold:\n",
    "            word_freq[word] = freq\n",
    "    return word_freq\n",
    "\n",
    "def count_of_bigrams( Tokens ,freqThreshold):\n",
    "        bigram_freq = defaultdict(int)\n",
    "\n",
    "        b = nltk.collocations.BigramCollocationFinder.from_words(Tokens)\n",
    "        b.apply_freq_filter(freqThreshold)\n",
    "       \n",
    "        for bigram, freq in b.ngram_fd.items():\n",
    "                bigram=\" \".join([bigram[0], bigram[1]])\n",
    "                bigram_freq[bigram] = freq\n",
    "        return bigram_freq\n",
    "    \n",
    "def pmi(w1, w2, unigram_freq , bigram_freq):\n",
    "\n",
    "    prob_word1 = unigram_freq[w1] / float(sum(unigram_freq.values()))\n",
    "    prob_word2 = unigram_freq[w2] / float(sum(unigram_freq.values()))\n",
    "    prob_word1_word2 = bigram_freq[\" \".join([w1, w2])] / float(sum(bigram_freq.values()))\n",
    "\n",
    "   \n",
    "    #print(\"PMI FOR W1 AND W2 \")\n",
    "    ##print(w1 , w2)\n",
    "    #print(\"Probability of w1 \")\n",
    "    #print( prob_word1)\n",
    "    #print(\"Probability of w2 \")\n",
    "    #print( prob_word2)\n",
    "    #print(\"Probability of w1 and w2 joint \")\n",
    "    #print( prob_word1_word2 )\n",
    "    if prob_word1_word2 == 0 :\n",
    "        return 0\n",
    "    #print(math.log(prob_word1_word2/float(prob_word1*prob_word2),2))\n",
    "    try:\n",
    "\n",
    "        return math.log(prob_word1_word2/float(prob_word1*prob_word2),2)\n",
    "\n",
    "    except: # Occurs when calculating PMI for Out-of-Vocab words.\n",
    "\n",
    "        return 0\n",
    "\n",
    "def get_pmi_mat(terms , windows ):\n",
    "    pmi_mat = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "    unigram_freq = count_of_single( terms , 0)\n",
    "    bigram_freq = count_of_bigrams( terms , 0)\n",
    "   \n",
    "    for window in windows:\n",
    "        #print(\" IN GET_PMI WINDOW IN WINDOWS\")\n",
    "        for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "            pmi_mat[w1][w2] = pmi( w1 , w2 ,  unigram_freq , bigram_freq )\n",
    "            \n",
    "            \n",
    "    return pmi_mat        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_final_weights( cosine_mat , pmi_mat ):\n",
    "    \n",
    "    final_weights = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "    for (i , j) , (k , l) in zip(cosine_mat.items() ,pmi_mat.items()):\n",
    "        for (a , b) , (c , d) in zip(j.items() , l.items()):\n",
    "                final_weights[i][a] = b*d \n",
    "              \n",
    "          \n",
    "    return final_weights\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_embedding_phrases(terms):\n",
    "    filename = 'C:/Users/hp/Word_embeddings/GoogleNews-vectors-negative300.bin'\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format( filename , binary=True)\n",
    "\n",
    "   \n",
    "    words_to_index_1 = {}\n",
    "    i = 0;\n",
    "    for a in terms:\n",
    "        for b in a.split():\n",
    "            if not b in words_to_index_1:\n",
    "                words_to_index_1[str(b)] = i\n",
    "                i = i + 1\n",
    "            else:\n",
    "                continue   \n",
    "    \"\"\"   \n",
    "    print(\"words_to_index in phrase embeddings\")\n",
    "    for k, v in words_to_index_1.items():\n",
    "        print(k, v)\n",
    "        \n",
    "    print(\" LENGTH OF WORS_TO_INDEX IS : \")\n",
    "    print(len(words_to_index_1))\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_matrix = np.zeros(( len(words_to_index_1) , 300 ))\n",
    "    for word , i in words_to_index_1.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "       \n",
    "    \n",
    "    phrases_to_index = {}\n",
    "    i = 0;\n",
    "    for a in terms:\n",
    "            if not a in phrases_to_index:\n",
    "                phrases_to_index[str(a)] = i\n",
    "                i = i + 1\n",
    "            else:\n",
    "                continue \n",
    "    \"\"\"    \n",
    "    print()    \n",
    "    print(\"phrases_to_index in phrase embeddings\")\n",
    "    print()\n",
    "    for k, v in phrases_to_index.items():\n",
    "        print(k, v) \n",
    "        \n",
    "    print(\" LENGTH OF PHRASES_TO_INDEX IS : \")\n",
    "    print(len( phrases_to_index ))    \n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_matrix_phrases = np.zeros(( len(words_to_index_1) , 300 ))\n",
    "    for phrase , i in phrases_to_index.items():\n",
    "        embedding_vector_final = np.zeros(300)\n",
    "        for a in phrase.split():\n",
    "            try:\n",
    "                embedding_vector = model[a] \n",
    "            except KeyError:\n",
    "                embedding_vector = None    \n",
    "            if embedding_vector is not None:\n",
    "                embedding_vector_final = np.add( embedding_vector_final , embedding_vector )   \n",
    "        embedding_matrix_phrases[i] = embedding_vector_final\n",
    "        \n",
    "           \n",
    "    return  embedding_matrix ,  words_to_index_1 , embedding_matrix_phrases ,  phrases_to_index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terms_to_semantic_network(terms, normalize='lemma', window_width=10, edge_weighting='cooc_freq' ,  phrases = False ):\n",
    "   \n",
    "    \"\"\"\n",
    "    Transform an ordered list of non-overlapping terms into a semantic network,\n",
    "    where each term is represented by a node with weighted edges linking it to\n",
    "    other terms that co-occur within ``window_width`` terms of itself.\n",
    "    Args:\n",
    "        terms (List[str] or List[``spacy.Token``])\n",
    "        normalize (str or Callable): If 'lemma', lemmatize terms; if 'lower',\n",
    "            lowercase terms; if false-y, use the form of terms as they appear\n",
    "            in ``terms``; if a callable, must accept a ``spacy.Token`` and return\n",
    "            a str, e.g. :func:`textacy.spacier.utils.get_normalized_text()`.\n",
    "            .. note:: This is applied to the elements of ``terms`` *only* if\n",
    "               it's a list of ``spacy.Token``.\n",
    "        window_width (int): Size of sliding window over ``terms`` that determines\n",
    "            which are said to co-occur. If 2, only immediately adjacent terms\n",
    "            have edges in the returned network.\n",
    "        edge_weighting ({'cooc_freq', 'binary'}): If 'cooc_freq', the nodes for\n",
    "            all co-occurring terms are connected by edges with weight equal to\n",
    "            the number of times they co-occurred within a sliding window;\n",
    "            if 'binary', all such edges have weight = 1.\n",
    "    Returns:\n",
    "        ``networkx.Graph``: Nodes in this network correspond to individual terms;\n",
    "        those that co-occur are connected by edges with weights determined\n",
    "        by ``edge_weighting``.\n",
    "    Notes:\n",
    "        - Be sure to filter out stopwords, punctuation, certain parts of speech, etc.\n",
    "          from the terms list before passing it to this function\n",
    "        - Multi-word terms, such as named entities and compound nouns, must be merged\n",
    "          into single strings or spacy.Tokens beforehand\n",
    "        - If terms are already strings, be sure to have normalized them so that\n",
    "          like terms are counted together; for example, by applying\n",
    "          :func:`textacy.spacier.utils.get_normalized_text()`\n",
    "    \"\"\"\n",
    "    if window_width < 2:\n",
    "        raise ValueError(\n",
    "            '`window_width` = {} is invalid; value must be >= 2'.format(window_width))\n",
    "    if not terms:\n",
    "        raise ValueError(\n",
    "            '`terms` = {} is invalid; it must contain at least 1 term '\n",
    "            'in the form of a string or spacy token'.format(terms))\n",
    "\n",
    "    # if len(terms) < window_width, cytoolz throws a StopIteration error\n",
    "    # which we don't want\n",
    "    if len(terms) < window_width:\n",
    "        LOGGER.info(\n",
    "            '`terms` has fewer items (%s) than the specified `window_width` (%s); '\n",
    "            'setting window width to %s',\n",
    "            len(terms), window_width, len(terms))\n",
    "        window_width = len(terms)\n",
    "\n",
    "    if isinstance(terms[0], compat.unicode_):\n",
    "        windows = itertoolz.sliding_window(window_width, terms)\n",
    "        to_return = list(windows)\n",
    "    elif isinstance(terms[0], SpacyToken):\n",
    "        if normalize == 'lemma':\n",
    "            windows = ((tok.lemma_ for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "        elif normalize == 'lower':\n",
    "            windows = ((tok.lower_ for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "        elif not normalize:\n",
    "            windows = ((tok.text for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "        else:\n",
    "            windows = ((normalize(tok) for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            'items in `terms` must be strings or spacy tokens, not {}'.format(type(terms[0])))\n",
    "\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "\n",
    "    \n",
    "    embedding_matrix_phrases = np.zeros(( len(terms) , 300 ))\n",
    "    \n",
    "    if edge_weighting == 'cooc_freq':\n",
    "        cooc_mat = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "        for window in to_return:\n",
    "            for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "                cooc_mat[w1][w2] += 1\n",
    "\n",
    "                               \n",
    "        graph.add_edges_from(\n",
    "            (w1, w2, {'weight': weight})\n",
    "            for w1, w2s in cooc_mat.items()\n",
    "            for w2, weight in w2s.items())\n",
    "    elif edge_weighting == 'binary':\n",
    "        graph.add_edges_from(\n",
    "            w1_w2 for window in to_return\n",
    "            for w1_w2 in itertools.combinations(window, 2))\n",
    "    \n",
    "    elif edge_weighting == 'embedding' and phrases == False:\n",
    "        embedding_matrix , word_to_index = get_word_embedding(terms)\n",
    "        cosine_mat = get_cosine_mat(to_return , embedding_matrix  , word_to_index )\n",
    "        pmi_mat = get_pmi_mat( terms , to_return )\n",
    "        final_weights = get_final_weights( cosine_mat , pmi_mat )\n",
    "        \n",
    "        for window in to_return:\n",
    "            for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "                graph.add_weighted_edges_from(\n",
    "                    (w1, w2,  weight)\n",
    "                    for w1, w2s in final_weights.items()\n",
    "                    for w2, weight in w2s.items())\n",
    "      \n",
    "    elif edge_weighting == 'embedding' and phrases == True:\n",
    "        embedding_matrix ,  words_to_index_1 , embedding_matrix_phrases ,  phrases_to_index =  get_word_embedding_phrases(terms)\n",
    "        cosine_mat = get_cosine_mat( to_return , embedding_matrix_phrases  , phrases_to_index )\n",
    "        pmi_mat = get_pmi_mat( terms , to_return )\n",
    "        final_weights = get_final_weights( cosine_mat , pmi_mat )\n",
    "   \n",
    "        for window in to_return:\n",
    "            for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "                graph.add_weighted_edges_from(\n",
    "                    (w1, w2,  weight)\n",
    "                    for w1, w2s in final_weights.items()\n",
    "                    for w2, weight in w2s.items())\n",
    "                \n",
    "    \n",
    "    #for (u , v , d) in graph.edges(data='weight'):\n",
    "        #if d < 0:\n",
    "            #d = d*(-1)\n",
    "           # graph[u][v]['weight'] = d\n",
    "            #print(\"CHANGED\")\n",
    "            #print()\n",
    "    \n",
    "  \n",
    "   \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgrank(doc, ngrams=(1, 2, 3, 4, 5, 6), normalize='lemma', window_width=1500,\n",
    "           n_keyterms=10, idf=None , edge_weighting = None , join_key_words = False):\n",
    "    \"\"\"\n",
    "    Extract key terms from a document using the [SGRank]_ algorithm.\n",
    "    Args:\n",
    "        doc (``textacy.Doc`` or ``spacy.Doc``)\n",
    "        ngrams (int or Set[int]): n of which n-grams to include; ``(1, 2, 3, 4, 5, 6)``\n",
    "                (default) includes all ngrams from 1 to 6; `2`\n",
    "                if only bigrams are wanted\n",
    "        normalize (str or callable): If 'lemma', lemmatize terms; if 'lower',\n",
    "            lowercase terms; if None, use the form of terms as they appeared in\n",
    "            ``doc``; if a callable, must accept a ``spacy.Span`` and return a str,\n",
    "            e.g. :func:`textacy.spacier.utils.get_normalized_text()`\n",
    "        window_width (int): Width of sliding window in which term\n",
    "            co-occurrences are determined to occur. Note: Larger values may\n",
    "            dramatically increase runtime, owing to the larger number of\n",
    "            co-occurrence combinations that must be counted.\n",
    "        n_keyterms (int or float): Number of top-ranked terms to return as\n",
    "            keyterms. If int, represents the absolute number; if float, must be\n",
    "            in the open interval (0.0, 1.0), and is converted to an integer by\n",
    "            ``int(round(len(doc) * n_keyterms))``\n",
    "        idf (dict): Mapping of ``normalize(term)`` to inverse document frequency\n",
    "            for re-weighting of unigrams (n-grams with n > 1 have df assumed = 1).\n",
    "            Results are typically better with idf information.\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: sorted list of top ``n_keyterms`` key terms and\n",
    "        their corresponding SGRank scores\n",
    "    Raises:\n",
    "        ValueError: If ``n_keyterms`` is a float but not in (0.0, 1.0] or\n",
    "            ``window_width`` < 2.\n",
    "    References:\n",
    "        .. [SGRank] Danesh, Sumner, and Martin. \"SGRank: Combining Statistical and\n",
    "           Graphical Methods to Improve the State of the Art in Unsupervised Keyphrase\n",
    "           Extraction\". Lexical and Computational Semantics (* SEM 2015) (2015): 117.\n",
    "    \"\"\"\n",
    "    n_toks = len(doc)\n",
    "    \n",
    "    if isinstance(n_keyterms, float):\n",
    "        if not 0.0 < n_keyterms <= 1.0:\n",
    "            raise ValueError('`n_keyterms` must be an int, or a float between 0.0 and 1.0')\n",
    "        n_keyterms = int(round(n_toks * n_keyterms))\n",
    "    \n",
    "    if window_width < 2:\n",
    "        raise ValueError('`window_width` must be >= 2')\n",
    "    \n",
    "    window_width = min(n_toks, window_width)\n",
    "    min_term_freq = min(n_toks // 1000, 4)\n",
    "    \n",
    "    if isinstance(ngrams, int):\n",
    "        ngrams = (ngrams,)\n",
    "\n",
    "    # build full list of candidate terms\n",
    "    # if inverse doc freqs available, include nouns, adjectives, and verbs;\n",
    "    # otherwise, just include nouns and adjectives\n",
    "    # (without IDF downweighting, verbs dominate the results in a bad way)\n",
    "    include_pos = {'NOUN', 'PROPN', 'ADJ', 'VERB'} if idf else {'NOUN', 'PROPN', 'ADJ'}\n",
    "    terms = itertoolz.concat(\n",
    "        extract.ngrams(doc, n, filter_stops=True, filter_punct=True, filter_nums=False,\n",
    "                       include_pos=include_pos, min_freq=min_term_freq)\n",
    "                       for n in ngrams)\n",
    "\n",
    "    #print(\" TERMS : N-GRAMS\")\n",
    "    #a = list(terms)\n",
    "    #print(a)\n",
    "    \n",
    "    \n",
    "    # get normalized term strings, as desired\n",
    "    # paired with positional index in document and length in a 3-tuple\n",
    "    if normalize == 'lemma':\n",
    "        terms = [(term.lemma_, term.start, len(term)) for term in terms]\n",
    "    elif normalize == 'lower':\n",
    "        terms = [(term.lower_, term.start, len(term)) for term in terms]\n",
    "    elif not normalize:\n",
    "        terms = [(term.text, term.start, len(term)) for term in terms]\n",
    "    else:\n",
    "        terms = [(normalize(term), term.start, len(term)) for term in terms]\n",
    "\n",
    "    # pre-filter terms to the top N ranked by TF or modified TF*IDF\n",
    "    n_prefilter_kts = max(3 * n_keyterms, 100)\n",
    "    term_text_counts = collections.Counter(term[0] for term in terms)\n",
    "    \n",
    "    \n",
    "    if idf:\n",
    "        mod_tfidfs = {\n",
    "            term: count * idf.get(term, 1) if ' ' not in term else count\n",
    "            for term, count in term_text_counts.items()}\n",
    "        terms_set = {\n",
    "            term for term, _\n",
    "            in sorted(mod_tfidfs.items(), key=operator.itemgetter(1), reverse=True)[:n_prefilter_kts]}\n",
    "    else:\n",
    "        terms_set = {term for term, _ in term_text_counts.most_common(n_prefilter_kts)}\n",
    "        terms = [term for term in terms if term[0] in terms_set]\n",
    "\n",
    "    term_weights = {}\n",
    "    seen_terms = set()\n",
    "    n_toks_plus_1 = n_toks + 1\n",
    "    for term in terms:\n",
    "        term_text = term[0]\n",
    "        # we only want the *first* occurrence of a unique term (by its text)\n",
    "        if term_text in seen_terms:\n",
    "            continue\n",
    "        seen_terms.add(term_text)\n",
    "        pos_first_occ_factor = math.log(n_toks_plus_1 / (term[1] + 1))\n",
    "        # TODO: assess how best to scale term len\n",
    "        term_len = math.sqrt(term[2])  # term[2]\n",
    "        term_count = term_text_counts[term_text]\n",
    "        subsum_count = sum(term_text_counts[t2] for t2 in terms_set\n",
    "                            if t2 != term_text and term_text in t2)\n",
    "        term_freq_factor = term_count - subsum_count\n",
    "        if idf and term[2] == 1:\n",
    "            term_freq_factor *= idf.get(term_text, 1)\n",
    "        term_weights[term_text] = term_freq_factor * pos_first_occ_factor * term_len\n",
    "    \n",
    "    \n",
    "    terms = [term for term in terms if term_weights[term[0]] > 0]\n",
    "\n",
    "    \n",
    "    if edge_weighting == \"embedding\":\n",
    "        #print(\"WOKING FINE\")  \n",
    "        phrases = []\n",
    "        words = []\n",
    "        \n",
    "        terms_words = [ term[0] for term in terms ]\n",
    "        for a in terms_words:\n",
    "            if len([b  for b in a.split()]) == 1:\n",
    "                words.append(a)\n",
    "            if len([b  for b in a.split()]) > 1:\n",
    "                phrases.append(a)        \n",
    "\n",
    "        ## If only words are needed\n",
    "        if edge_weighting == 'embedding' and join_key_words == False:    \n",
    "            graph = terms_to_semantic_network( words , window_width = window_width, edge_weighting = edge_weighting , phrases = False)\n",
    "    \n",
    "        ## If keyphrases are needed\n",
    "        elif edge_weighting == 'embedding' and join_key_words == True:\n",
    "            graph = terms_to_semantic_network( phrases , window_width = window_width, edge_weighting = edge_weighting , phrases = True)  \n",
    "    \n",
    "        ## Using PageRank for ranking keywords/keyphrases\n",
    "        word_ranks = nx.pagerank_scipy(graph , max_iter = 100 ,  weight = 'weight')\n",
    "        return [(word, score) for word, score in\n",
    "                sorted(word_ranks.items(), key=operator.itemgetter(1), reverse=True)[:n_keyterms]]\n",
    "        \n",
    "\n",
    "    # compute term weights from statistical attributes:\n",
    "    # not subsumed frequency, position of first occurrence, and num words\n",
    "    \n",
    "    else : \n",
    "        n_coocs = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "        sum_logdists = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "\n",
    "        # iterate over windows\n",
    "        log_ = math.log  # localize this, for performance\n",
    "        for start_ind in compat.range_(n_toks):\n",
    "            end_ind = start_ind + window_width\n",
    "            window_terms = (term for term in terms\n",
    "                            if start_ind <= term[1] <= end_ind)\n",
    "            # get all token combinations within window\n",
    "            for t1, t2 in itertools.combinations(window_terms, 2):\n",
    "                n_coocs[t1[0]][t2[0]] += 1\n",
    "                sum_logdists[t1[0]][t2[0]] += log_(window_width / max(abs(t1[1] - t2[1]), 1))\n",
    "            if end_ind > n_toks:\n",
    "                break\n",
    "\n",
    "            # compute edge weights between co-occurring terms (nodes)\n",
    "        edge_weights = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "        for t1, t2s in sum_logdists.items():\n",
    "            for t2 in t2s:\n",
    "                edge_weights[t1][t2] = ((1.0 + sum_logdists[t1][t2]) / n_coocs[t1][t2]) * term_weights[t1] * term_weights[t2]\n",
    "            # normalize edge weights by sum of outgoing edge weights per term (node)\n",
    "                norm_edge_weights = []\n",
    "            for t1, t2s in edge_weights.items():\n",
    "                sum_edge_weights = sum(t2s.values())\n",
    "                norm_edge_weights.extend((t1, t2, {'weight': weight / sum_edge_weights})\n",
    "                                         for t2, weight in t2s.items())\n",
    "\n",
    "            # build the weighted directed graph from edges, rank nodes by pagerank\n",
    "            graph = nx.DiGraph()\n",
    "            graph.add_edges_from(norm_edge_weights)\n",
    "            term_ranks = nx.pagerank_scipy(graph)\n",
    "\n",
    "            return sorted(term_ranks.items(), key=operator.itemgetter(1, 0), reverse=True)[:n_keyterms]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = sgrank(doc, ngrams=(1, 2, 3, 4, 5, 6), normalize='lemma', window_width = 2, n_keyterms = 20, idf = None , edge_weighting = \"embedding\" , join_key_words = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine learning', 0.12780771437464766),\n",
       " ('computer science', 0.05221932114882506),\n",
       " ('statistical technique', 0.05221932114882506),\n",
       " ('computer system', 0.05221932114882506),\n",
       " ('specific task', 0.05221932114882506),\n",
       " ('sample input', 0.05221932114882506),\n",
       " ('arthur samuel', 0.05221932114882506),\n",
       " ('pattern recognition', 0.05221932114882506),\n",
       " ('computational learning', 0.05221932114882506),\n",
       " ('learning theory', 0.05221932114882506),\n",
       " ('program instruction', 0.05221932114882506),\n",
       " ('explicit algorithm', 0.05221932114882506),\n",
       " ('good performance', 0.05221932114882506),\n",
       " ('email filtering', 0.05221932114882506),\n",
       " ('network intruder', 0.05221932114882506),\n",
       " ('computer vision', 0.04710912678014121),\n",
       " ('static program', 0.03647922142751169),\n",
       " ('artificial intelligence', 0.02646188209902444),\n",
       " ('programming explicit', 0.023238661062800217),\n",
       " ('example application', 0.00783289817232376)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = sgrank(doc, ngrams=(1, 2, 3, 4, 5, 6), normalize='lemma', window_width = 2, n_keyterms = 20, idf = None , edge_weighting = \"embedding\" , join_key_words = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tie', 0.04348985531602103),\n",
       " ('model', 0.04181281865616307),\n",
       " ('strong', 0.040243266227312134),\n",
       " ('analytic', 0.034382534800116035),\n",
       " ('analytical', 0.03400618093137236),\n",
       " ('application', 0.03357994074430049),\n",
       " ('mining', 0.03338276537803992),\n",
       " ('relationship', 0.032288908811599),\n",
       " ('use', 0.03228890881159899),\n",
       " ('reliable', 0.032288908811598974),\n",
       " ('datum', 0.0317891929019165),\n",
       " ('algorithm', 0.02906417551804707),\n",
       " ('domain', 0.028654509511509125),\n",
       " ('repeatable', 0.028354816324690106),\n",
       " ('data', 0.027011533007003728),\n",
       " ('theory', 0.02634262575431781),\n",
       " ('prediction', 0.025372802071835565),\n",
       " ('subfield', 0.02262360971499223),\n",
       " ('trend', 0.02224273635553439),\n",
       " ('difficult', 0.02212389380530973)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Without embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyterms_without_embedding = sgrank(doc, ngrams=(1, 2, 3, 4, 5, 6), normalize='lemma', window_width = 2, n_keyterms = 20, idf = None , join_key_words = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine learning', 0.27232895809719715),\n",
       " ('datum', 0.19858037029257036),\n",
       " ('computer science', 0.18047109960756447),\n",
       " ('computer', 0.1776795203062141),\n",
       " ('field', 0.17094005169645402)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyterms_without_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
