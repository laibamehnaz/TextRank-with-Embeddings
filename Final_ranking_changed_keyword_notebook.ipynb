{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "import networkx as nx\n",
    "from cytoolz import itertoolz\n",
    "from spacy.tokens.span import Span as SpacySpan\n",
    "from spacy.tokens.token import Token as SpacyToken\n",
    "\n",
    "import compat\n",
    "import extract\n",
    "import vsm\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import spacy \n",
    "import textacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text used\n",
    "Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to \"learn\" (e.g., progressively improve performance on a specific task) with data, without being explicitly programmed.The name machine learning was coined in 1959 by Arthur Samuel.Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data – such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions, through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders, and computer vision.Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining, where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to \"produce reliable, repeatable decisions and results\" and uncover \"hidden insights\" through learning from historical relationships and trends in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to learn (e.g., progressively improve performance on a specific task) with data, without being explicitly programmed.The name machine learning was coined in 1959 by Arthur Samuel.Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data – such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions, through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders, and computer vision.Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining, where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to produce reliable, repeatable decisions and results and uncover hidden insights through learning from historical relationships and trends in the data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(u\"Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to learn (e.g., progressively improve performance on a specific task) with data, without being explicitly programmed.The name machine learning was coined in 1959 by Arthur Samuel.Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data – such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions, through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders, and computer vision.Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining, where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning.Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to produce reliable, repeatable decisions and results and uncover hidden insights through learning from historical relationships and trends in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def key_terms_from_semantic_network(doc, normalize='lemma',\n",
    "                                    window_width=2, edge_weighting='binary',\n",
    "                                    ranking_algo='pagerank', join_key_words=False,\n",
    "                                    n_keyterms=10, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract key terms from a document by ranking nodes in a semantic network of\n",
    "    terms, connected by edges and weights specified by parameters.\n",
    "    Args:\n",
    "        doc (``textacy.Doc`` or ``spacy.Doc``)\n",
    "        normalize (str or callable): if 'lemma', lemmatize terms; if 'lower',\n",
    "            lowercase terms; if None, use the form of terms as they appeared in\n",
    "            ``doc``; if a callable, must accept a ``spacy.Token`` and return a str,\n",
    "            e.g. :func:`textacy.spacier.utils.get_normalized_text()`\n",
    "        window_width (int): width of sliding window in which term\n",
    "            co-occurrences are said to occur\n",
    "        edge_weighting ('binary', 'cooc_freq'}): method used to\n",
    "            determine weights of edges between nodes in the semantic network;\n",
    "            if 'binary', edge weight is set to 1 for any two terms co-occurring\n",
    "            within `window_width` terms; if 'cooc_freq', edge weight is set to\n",
    "            the number of times that any two terms co-occur\n",
    "        ranking_algo ({'pagerank', 'divrank', 'bestcoverage'}):\n",
    "            algorithm with which to rank nodes in the semantic network;\n",
    "            `pagerank` is the canonical (and default) algorithm, but it prioritizes\n",
    "            node centrality at the expense of node diversity; the other two\n",
    "            attempt to balance centrality with diversity\n",
    "        join_key_words (bool): if True, join consecutive key words\n",
    "            together into longer key terms, taking the sum of the constituent words'\n",
    "            scores as the joined key term's combined score\n",
    "        n_keyterms (int or float): if int, number of top-ranked terms\n",
    "            to return as keyterms; if float, must be in the open interval (0, 1),\n",
    "            is converted to an integer by ``round(len(doc) * n_keyterms)``\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: sorted list of top ``n_keyterms`` key terms and\n",
    "        their corresponding ranking scores\n",
    "    Raises:\n",
    "        ValueError: if ``n_keyterms`` is a float but not in (0.0, 1.0]\n",
    "    \"\"\"\n",
    "    if isinstance(n_keyterms, float):\n",
    "        if not 0.0 < n_keyterms <= 1.0:\n",
    "            raise ValueError('`n_keyterms` must be an int, or a float between 0.0 and 1.0')\n",
    "        n_keyterms = int(round(len(doc) * n_keyterms))\n",
    "\n",
    "        \n",
    "        \n",
    "    if edge_weighting == 'binary':\n",
    "        include_pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "        if normalize == 'lemma':\n",
    "            word_list = [word.lemma_ for word in doc]\n",
    "            good_word_list = [word.lemma_ for word in doc\n",
    "                              if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "        elif normalize == 'lower':\n",
    "            word_list = [word.lower_ for word in doc]\n",
    "            good_word_list = [word.lower_ for word in doc\n",
    "                              if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "        elif not normalize:\n",
    "            word_list = [word.text for word in doc]\n",
    "            good_word_list = [word.text for word in doc\n",
    "                              if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "        else:\n",
    "            word_list = [normalize(word) for word in doc]\n",
    "            good_word_list = [normalize(word) for word in doc\n",
    "                                  if not word.is_stop and not word.is_punct and word.pos_ in include_pos]\n",
    "        \n",
    "        # HACK: omit empty strings, which happen as a bug in spacy as of v1.5\n",
    "        # and may well happen with ``normalize`` as a callable\n",
    "        # an empty string should never be considered a keyterm\n",
    "    \n",
    "    \n",
    "        good_word_list = [word for word in good_word_list if word]    \n",
    "        \n",
    "        graph = terms_to_semantic_network( good_word_list, window_width = window_width, edge_weighting = edge_weighting , phrases = join_key_words )\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        if ranking_algo == 'pagerank':\n",
    "            word_ranks = nx.pagerank_scipy(graph , max_iter = 100 ,  weight = 'weight')\n",
    "        elif ranking_algo == 'divrank':\n",
    "            word_ranks = rank_nodes_by_divrank(  graph, r=None, lambda_=kwargs.get('lambda_', 0.5), alpha=kwargs.get('alpha', 0.5))\n",
    "        elif ranking_algo == 'bestcoverage':\n",
    "            word_ranks = rank_nodes_by_bestcoverage( graph, k=n_keyterms, c=kwargs.get('c', 1), alpha=kwargs.get('alpha', 1.0))\n",
    "            \n",
    "            \n",
    "        if edge_weighting == 'binary' and join_key_words is False:\n",
    "            return [(word, score) for word, score in\n",
    "                    sorted(word_ranks.items(), key=operator.itemgetter(1), reverse=True)[:n_keyterms]]\n",
    "    \n",
    "        elif edge_weighting == 'binary' and join_key_words is True:   \n",
    "            top_n = int(0.25 * len(word_ranks))\n",
    "            top_word_ranks = {word: rank for word, rank in sorted(word_ranks.items(), key=operator.itemgetter(1), reverse=True)[:top_n]}\n",
    "            # join consecutive key words into key terms\n",
    "            seen_joined_key_terms = set()\n",
    "            joined_key_terms = []\n",
    "            for key, group in itertools.groupby(word_list, lambda word: word in top_word_ranks):\n",
    "                if key is True:\n",
    "                    words = list(group)\n",
    "                    term = ' '.join(words)\n",
    "                    if term in seen_joined_key_terms:\n",
    "                        continue\n",
    "                    seen_joined_key_terms.add(term)\n",
    "                    joined_key_terms.append((term, sum(word_ranks[word] for word in words)))        \n",
    "            return sorted(joined_key_terms, key=operator.itemgetter(1, 0), reverse=True)[:n_keyterms]\n",
    "        \n",
    "        \n",
    "    elif edge_weighting == 'embedding':\n",
    "        good_word_list_noun_chunks = extract.noun_chunks(doc , drop_determiners = True, min_freq = 1 )\n",
    "        good_word_list_ner = extract.named_entities( doc, include_types = None, exclude_types = None, drop_determiners = True, min_freq = 1 )\n",
    "        \n",
    "        list_noun_chunks = []\n",
    "        for a in good_word_list_noun_chunks :\n",
    "            list_noun_chunks.append(str(a))\n",
    "            \n",
    "        list_ner = []\n",
    "        for a in good_word_list_ner :\n",
    "            list_ner.append(str(a)) \n",
    "        \n",
    "        list_nc_ner = [] \n",
    "        for a in list_noun_chunks:\n",
    "            list_nc_ner.append(str(a))\n",
    "        for a in list_ner:\n",
    "            list_nc_ner.append(str(a))    \n",
    "        \n",
    "        phrases = []\n",
    "        words = []\n",
    "\n",
    "        for a in list_nc_ner:\n",
    "            if len([b  for b in a.split()]) == 1:\n",
    "                words.append(a)\n",
    "            if len([b  for b in a.split()]) > 1:\n",
    "                phrases.append(a)        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if edge_weighting == 'embedding' and join_key_words == False:    \n",
    "            graph = terms_to_semantic_network( words , window_width = window_width, edge_weighting = edge_weighting , phrases = join_key_words)\n",
    "    \n",
    "        elif edge_weighting == 'embedding' and join_key_words == True:\n",
    "            graph = terms_to_semantic_network( phrases , window_width = window_width, edge_weighting = edge_weighting , phrases = join_key_words)\n",
    "    \n",
    "         \n",
    "        if ranking_algo == 'pagerank':\n",
    "            word_ranks = nx.pagerank_scipy(graph , max_iter = 100 ,  weight = 'weight')\n",
    "        elif ranking_algo == 'divrank':\n",
    "            word_ranks = rank_nodes_by_divrank(  graph, r=None, lambda_=kwargs.get('lambda_', 0.5), alpha=kwargs.get('alpha', 0.5))\n",
    "        elif ranking_algo == 'bestcoverage':\n",
    "            word_ranks = rank_nodes_by_bestcoverage( graph, k=n_keyterms, c=kwargs.get('c', 1), alpha=kwargs.get('alpha', 1.))\n",
    "            \n",
    "        \n",
    "        return [(word, score) for word, score in\n",
    "                sorted(word_ranks.items(), key=operator.itemgetter(1), reverse=True)[:n_keyterms]]\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords with embeddings and phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine learning', 0.061804724756885566),\n",
       " ('Machine learning', 0.05834873746330665),\n",
       " ('computer science', 0.05100664234397605),\n",
       " ('data scientists', 0.04700542195976324),\n",
       " ('data mining', 0.03611174636299395),\n",
       " ('computing tasks', 0.03405591738489863),\n",
       " ('sample inputs', 0.032206119162640906),\n",
       " ('Arthur Samuel', 0.032206119162640906),\n",
       " ('pattern recognition', 0.032206119162640906),\n",
       " ('historical relationships', 0.032206119162640906)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = key_terms_from_semantic_network( doc, normalize='lemma',\n",
    "                                    window_width=2, edge_weighting='embedding',\n",
    "                                    ranking_algo='pagerank', join_key_words = True,\n",
    "                                    n_keyterms=10)   \n",
    "terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords with binary and phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computational learning theory', 0.0712335712229255),\n",
       " ('machine learning', 0.0647414744052546),\n",
       " ('datum analytic', 0.06261682489695981),\n",
       " ('datum', 0.047329322036000925),\n",
       " ('learning', 0.03934194663842974),\n",
       " ('computer', 0.0327936669527043),\n",
       " ('method use', 0.03241966295654762),\n",
       " ('algorithm', 0.027124563106213542),\n",
       " ('prediction', 0.026868210400336164),\n",
       " ('model', 0.02331510184695999)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = key_terms_from_semantic_network( doc, normalize='lemma',\n",
    "                                    window_width=2, edge_weighting='binary',\n",
    "                                    ranking_algo='pagerank', join_key_words = True,\n",
    "                                    n_keyterms=10)   \n",
    "terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords with embeddings and not phrases just words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prediction', 0.08434577002010366),\n",
       " ('field', 0.07757674791782004),\n",
       " ('study', 0.07664986997651164),\n",
       " ('methods', 0.0641509069880738),\n",
       " ('algorithms', 0.06415090698807378),\n",
       " ('results', 0.06415090698807378),\n",
       " ('theory', 0.05265271718110291),\n",
       " ('detection', 0.046259448947606786),\n",
       " ('predictions', 0.045843984357979574),\n",
       " ('ability', 0.04395604395604395)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = key_terms_from_semantic_network( doc, normalize='lemma',\n",
    "                                    window_width=2, edge_weighting='embedding',\n",
    "                                    ranking_algo='pagerank', join_key_words = False,\n",
    "                                    n_keyterms=10)   \n",
    "terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords with binary and not phrases just words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('datum', 0.047329322036000925),\n",
       " ('learning', 0.03934194663842974),\n",
       " ('computer', 0.0327936669527043),\n",
       " ('algorithm', 0.027124563106213542),\n",
       " ('prediction', 0.026868210400336164),\n",
       " ('machine', 0.02539952776682486),\n",
       " ('model', 0.02331510184695999),\n",
       " ('performance', 0.018365129915958473),\n",
       " ('field', 0.01828901389385712),\n",
       " ('application', 0.017626060335524624)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = key_terms_from_semantic_network( doc, normalize='lemma',\n",
    "                                    window_width=2, edge_weighting='binary',\n",
    "                                    ranking_algo='pagerank', join_key_words = False,\n",
    "                                    n_keyterms=10)   \n",
    "terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_embedding(terms):\n",
    "    filename = 'C:/Users/hp/Word_embeddings/GoogleNews-vectors-negative300.bin'\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format( filename , binary=True)\n",
    "\n",
    "    words_to_index = {}\n",
    "    i = 0;\n",
    "    for word in terms:\n",
    "        if not word in words_to_index:\n",
    "            words_to_index[str(word)] = i\n",
    "            i = i + 1\n",
    "        else:\n",
    "            continue\n",
    "   \n",
    "   \n",
    "    embedding_matrix = np.zeros(( len(words_to_index) , 300 ))\n",
    "    for word , i in words_to_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "    return (embedding_matrix , words_to_index)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "    to the definition of the dot product\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0 :\n",
    "        return 0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def get_cosine_mat(windows , embedding_matrix , words_to_index ):\n",
    "    from scipy import spatial\n",
    "    cosine_mat = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "  \n",
    "    for window in windows:\n",
    "        for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "               cosine_mat[w1][w2] = cos_sim(embedding_matrix[words_to_index[w1]] , embedding_matrix[words_to_index[w2]] )\n",
    "           \n",
    "            \n",
    "    return cosine_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def count_of_single( Tokens , freqThreshold):          \n",
    "    word_freq = defaultdict(int)\n",
    "    fdist = defaultdict(int)\n",
    "    for a in Tokens:\n",
    "        fdist[a] += 1 \n",
    "    \n",
    "    for word , freq in sorted(fdist.items(), key=lambda k__v: (k__v[1],k__v[0])):\n",
    "        if freq > freqThreshold:\n",
    "            word_freq[word] = freq\n",
    "    return word_freq\n",
    "\n",
    "def count_of_bigrams( Tokens ,freqThreshold):\n",
    "        bigram_freq = defaultdict(int)\n",
    "\n",
    "        b = nltk.collocations.BigramCollocationFinder.from_words(Tokens)\n",
    "        b.apply_freq_filter(freqThreshold)\n",
    "       \n",
    "        for bigram, freq in b.ngram_fd.items():\n",
    "                bigram=\" \".join([bigram[0], bigram[1]])\n",
    "                bigram_freq[bigram] = freq\n",
    "        return bigram_freq\n",
    "    \n",
    "def pmi(w1, w2, unigram_freq , bigram_freq):\n",
    "\n",
    "    prob_word1 = unigram_freq[w1] / float(sum(unigram_freq.values()))\n",
    "    prob_word2 = unigram_freq[w2] / float(sum(unigram_freq.values()))\n",
    "    prob_word1_word2 = bigram_freq[\" \".join([w1, w2])] / float(sum(bigram_freq.values()))\n",
    "\n",
    "   \n",
    "    #print(\"PMI FOR W1 AND W2 \")\n",
    "    ##print(w1 , w2)\n",
    "    #print(\"Probability of w1 \")\n",
    "    #print( prob_word1)\n",
    "    #print(\"Probability of w2 \")\n",
    "    #print( prob_word2)\n",
    "    #print(\"Probability of w1 and w2 joint \")\n",
    "    #print( prob_word1_word2 )\n",
    "    if prob_word1_word2 == 0 :\n",
    "        return 0\n",
    "    #print(math.log(prob_word1_word2/float(prob_word1*prob_word2),2))\n",
    "    try:\n",
    "\n",
    "        return math.log(prob_word1_word2/float(prob_word1*prob_word2),2)\n",
    "\n",
    "    except: # Occurs when calculating PMI for Out-of-Vocab words.\n",
    "\n",
    "        return 0\n",
    "\n",
    "def get_pmi_mat(terms , windows ):\n",
    "    pmi_mat = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "    unigram_freq = count_of_single( terms , 0)\n",
    "    bigram_freq = count_of_bigrams( terms , 0)\n",
    "   \n",
    "    for window in windows:\n",
    "        #print(\" IN GET_PMI WINDOW IN WINDOWS\")\n",
    "        for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "            pmi_mat[w1][w2] = pmi( w1 , w2 ,  unigram_freq , bigram_freq )\n",
    "            \n",
    "            \n",
    "    return pmi_mat        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_final_weights( cosine_mat , pmi_mat ):\n",
    "    \n",
    "    final_weights = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "    for (i , j) , (k , l) in zip(cosine_mat.items() ,pmi_mat.items()):\n",
    "        for (a , b) , (c , d) in zip(j.items() , l.items()):\n",
    "                final_weights[i][a] = b*d \n",
    "              \n",
    "          \n",
    "    return final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_embedding_phrases(terms):\n",
    "    filename = 'C:/Users/hp/Word_embeddings/GoogleNews-vectors-negative300.bin'\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format( filename , binary=True)\n",
    "\n",
    "   \n",
    "    words_to_index_1 = {}\n",
    "    i = 0;\n",
    "    for a in terms:\n",
    "        for b in a.split():\n",
    "            if not b in words_to_index_1:\n",
    "                words_to_index_1[str(b)] = i\n",
    "                i = i + 1\n",
    "            else:\n",
    "                continue   \n",
    "    \"\"\"   \n",
    "    print(\"words_to_index in phrase embeddings\")\n",
    "    for k, v in words_to_index_1.items():\n",
    "        print(k, v)\n",
    "        \n",
    "    print(\" LENGTH OF WORS_TO_INDEX IS : \")\n",
    "    print(len(words_to_index_1))\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_matrix = np.zeros(( len(words_to_index_1) , 300 ))\n",
    "    for word , i in words_to_index_1.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "       \n",
    "    \n",
    "    phrases_to_index = {}\n",
    "    i = 0;\n",
    "    for a in terms:\n",
    "            if not a in phrases_to_index:\n",
    "                phrases_to_index[str(a)] = i\n",
    "                i = i + 1\n",
    "            else:\n",
    "                continue \n",
    "    \"\"\"    \n",
    "    print()    \n",
    "    print(\"phrases_to_index in phrase embeddings\")\n",
    "    print()\n",
    "    for k, v in phrases_to_index.items():\n",
    "        print(k, v) \n",
    "        \n",
    "    print(\" LENGTH OF PHRASES_TO_INDEX IS : \")\n",
    "    print(len( phrases_to_index ))    \n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_matrix_phrases = np.zeros(( len(words_to_index_1) , 300 ))\n",
    "    for phrase , i in phrases_to_index.items():\n",
    "        embedding_vector_final = np.zeros(300)\n",
    "        for a in phrase.split():\n",
    "            try:\n",
    "                embedding_vector = model[a] \n",
    "            except KeyError:\n",
    "                embedding_vector = None    \n",
    "            if embedding_vector is not None:\n",
    "                embedding_vector_final = np.add( embedding_vector_final , embedding_vector )   \n",
    "        embedding_matrix_phrases[i] = embedding_vector_final\n",
    "        \n",
    "           \n",
    "    return  embedding_matrix ,  words_to_index_1 , embedding_matrix_phrases ,  phrases_to_index\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fucntion for building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terms_to_semantic_network(terms, normalize='lemma', window_width=10, edge_weighting='cooc_freq' ,  phrases = False ):\n",
    "   \n",
    "    \"\"\"\n",
    "    Transform an ordered list of non-overlapping terms into a semantic network,\n",
    "    where each term is represented by a node with weighted edges linking it to\n",
    "    other terms that co-occur within ``window_width`` terms of itself.\n",
    "    Args:\n",
    "        terms (List[str] or List[``spacy.Token``])\n",
    "        normalize (str or Callable): If 'lemma', lemmatize terms; if 'lower',\n",
    "            lowercase terms; if false-y, use the form of terms as they appear\n",
    "            in ``terms``; if a callable, must accept a ``spacy.Token`` and return\n",
    "            a str, e.g. :func:`textacy.spacier.utils.get_normalized_text()`.\n",
    "            .. note:: This is applied to the elements of ``terms`` *only* if\n",
    "               it's a list of ``spacy.Token``.\n",
    "        window_width (int): Size of sliding window over ``terms`` that determines\n",
    "            which are said to co-occur. If 2, only immediately adjacent terms\n",
    "            have edges in the returned network.\n",
    "        edge_weighting ({'cooc_freq', 'binary'}): If 'cooc_freq', the nodes for\n",
    "            all co-occurring terms are connected by edges with weight equal to\n",
    "            the number of times they co-occurred within a sliding window;\n",
    "            if 'binary', all such edges have weight = 1.\n",
    "    Returns:\n",
    "        ``networkx.Graph``: Nodes in this network correspond to individual terms;\n",
    "        those that co-occur are connected by edges with weights determined\n",
    "        by ``edge_weighting``.\n",
    "    Notes:\n",
    "        - Be sure to filter out stopwords, punctuation, certain parts of speech, etc.\n",
    "          from the terms list before passing it to this function\n",
    "        - Multi-word terms, such as named entities and compound nouns, must be merged\n",
    "          into single strings or spacy.Tokens beforehand\n",
    "        - If terms are already strings, be sure to have normalized them so that\n",
    "          like terms are counted together; for example, by applying\n",
    "          :func:`textacy.spacier.utils.get_normalized_text()`\n",
    "    \"\"\"\n",
    "    if window_width < 2:\n",
    "        raise ValueError(\n",
    "            '`window_width` = {} is invalid; value must be >= 2'.format(window_width))\n",
    "    if not terms:\n",
    "        raise ValueError(\n",
    "            '`terms` = {} is invalid; it must contain at least 1 term '\n",
    "            'in the form of a string or spacy token'.format(terms))\n",
    "\n",
    "    # if len(terms) < window_width, cytoolz throws a StopIteration error\n",
    "    # which we don't want\n",
    "    if len(terms) < window_width:\n",
    "        LOGGER.info(\n",
    "            '`terms` has fewer items (%s) than the specified `window_width` (%s); '\n",
    "            'setting window width to %s',\n",
    "            len(terms), window_width, len(terms))\n",
    "        window_width = len(terms)\n",
    "\n",
    "    if isinstance(terms[0], compat.unicode_):\n",
    "        windows = itertoolz.sliding_window(window_width, terms)\n",
    "        to_return = list(windows)\n",
    "    elif isinstance(terms[0], SpacyToken):\n",
    "        if normalize == 'lemma':\n",
    "            windows = ((tok.lemma_ for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "        elif normalize == 'lower':\n",
    "            windows = ((tok.lower_ for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "        elif not normalize:\n",
    "            windows = ((tok.text for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "        else:\n",
    "            windows = ((normalize(tok) for tok in window)\n",
    "                       for window in itertoolz.sliding_window(window_width, terms))\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            'items in `terms` must be strings or spacy tokens, not {}'.format(type(terms[0])))\n",
    "\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "\n",
    "    \n",
    "    embedding_matrix_phrases = np.zeros(( len(terms) , 300 ))\n",
    "    \n",
    "    if edge_weighting == 'cooc_freq':\n",
    "        cooc_mat = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "        for window in to_return:\n",
    "            for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "                cooc_mat[w1][w2] += 1\n",
    "\n",
    "                               \n",
    "        graph.add_edges_from(\n",
    "            (w1, w2, {'weight': weight})\n",
    "            for w1, w2s in cooc_mat.items()\n",
    "            for w2, weight in w2s.items())\n",
    "    elif edge_weighting == 'binary':\n",
    "        graph.add_edges_from(\n",
    "            w1_w2 for window in to_return\n",
    "            for w1_w2 in itertools.combinations(window, 2))\n",
    "    \n",
    "    elif edge_weighting == 'embedding' and phrases == False:\n",
    "        embedding_matrix , word_to_index = get_word_embedding(terms)\n",
    "        cosine_mat = get_cosine_mat(to_return , embedding_matrix  , word_to_index )\n",
    "        pmi_mat = get_pmi_mat( terms , to_return )\n",
    "        final_weights = get_final_weights( cosine_mat , pmi_mat )\n",
    "        \n",
    "        for window in to_return:\n",
    "            for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "                graph.add_weighted_edges_from(\n",
    "                    (w1, w2,  weight)\n",
    "                    for w1, w2s in final_weights.items()\n",
    "                    for w2, weight in w2s.items())\n",
    "      \n",
    "    elif edge_weighting == 'embedding' and phrases == True:\n",
    "        embedding_matrix ,  words_to_index_1 , embedding_matrix_phrases ,  phrases_to_index =  get_word_embedding_phrases(terms)\n",
    "        cosine_mat = get_cosine_mat( to_return , embedding_matrix_phrases  , phrases_to_index )\n",
    "        pmi_mat = get_pmi_mat( terms , to_return )\n",
    "        final_weights = get_final_weights( cosine_mat , pmi_mat )\n",
    "   \n",
    "        for window in to_return:\n",
    "            for w1, w2 in itertools.combinations(sorted(window), 2):\n",
    "                graph.add_weighted_edges_from(\n",
    "                    (w1, w2,  weight)\n",
    "                    for w1, w2s in final_weights.items()\n",
    "                    for w2, weight in w2s.items())\n",
    "                \n",
    "    \n",
    "    #for (u , v , d) in graph.edges(data='weight'):\n",
    "        #if d < 0:\n",
    "            #d = d*(-1)\n",
    "           # graph[u][v]['weight'] = d\n",
    "            #print(\"CHANGED\")\n",
    "            #print()\n",
    "    \n",
    "  \n",
    "   \n",
    "    return graph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
