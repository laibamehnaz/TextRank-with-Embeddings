{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "import textacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import SnowballStemmer as Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import networkx as nx\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Used\n",
    "A blockchain,originally block chain, is a growing list of records, called blocks, which are linked using cryptography.Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a merkle tree root hash). By design, a blockchain is resistant to modification of the data. It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\". For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for inter-node communication and validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without alteration of all subsequent blocks, which requires consensus of the network majority. Although blockchain records are not unalterable, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been claimed with a blockchain. Blockchain was invented by Satoshi Nakamoto in 2008 to serve as the public transaction ledger of the cryptocurrency bitcoin. \n",
    "The invention of the blockchain for bitcoin made it the first digital currency to solve the double-spending problem without the need of a trusted authority or central server. The bitcoin design has inspired other applications, and blockchains which are readable by the public are widely used by cryptocurrencies. Private blockchains have been proposed for business use. Some marketing of blockchains has been called \"snake oil\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"A blockchain,originally block chain, is a growing list of records, called blocks, which are linked using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a merkle tree root hash). By design, a blockchain is resistant to modification of the data. It is an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way. For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for inter-node communication and validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without alteration of all subsequent blocks, which requires consensus of the network majority. Although blockchain records are not unalterable, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been claimed with a blockchain. Blockchain was invented by Satoshi Nakamoto in 2008 to serve as the public transaction ledger of the cryptocurrency bitcoin. The invention of the blockchain for bitcoin made it the first digital currency to solve the double-spending problem without the need of a trusted authority or central server. The bitcoin design has inspired other applications, and blockchains which are readable by the public are widely used by cryptocurrencies. Private blockchains have been proposed for business use. Some marketing of blockchains has been called snake oil.\"              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(u\"A blockchain,originally block chain, is a growing list of records, called blocks, which are linked using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a merkle tree root hash). By design, a blockchain is resistant to modification of the data. It is an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way. For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for inter-node communication and validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without alteration of all subsequent blocks, which requires consensus of the network majority. Although blockchain records are not unalterable, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been claimed with a blockchain. Blockchain was invented by Satoshi Nakamoto in 2008 to serve as the public transaction ledger of the cryptocurrency bitcoin. The invention of the blockchain for bitcoin made it the first digital currency to solve the double-spending problem without the need of a trusted authority or central server. The bitcoin design has inspired other applications, and blockchains which are readable by the public are widely used by cryptocurrencies. Private blockchains have been proposed for business use. Some marketing of blockchains has been called snake oil.\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Candidate(object):\n",
    "    \"\"\" The keyphrase candidate data structure. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.surface_forms = []\n",
    "        \"\"\" the surface forms of the candidate. \"\"\"\n",
    "\n",
    "        self.offsets = []\n",
    "        \"\"\" the offsets of the surface forms. \"\"\"\n",
    "\n",
    "        self.sentence_ids = []\n",
    "        \"\"\" the sentence id of each surface form. \"\"\"\n",
    "\n",
    "        self.pos_patterns = []\n",
    "        \"\"\" the Part-Of-Speech patterns of the candidate. \"\"\"\n",
    "\n",
    "        self.lexical_form = []\n",
    "        \"\"\" the lexical form of the candidate. \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_candidates(candidates , topics):\n",
    "    \"\"\"Vectorize the keyphrase candidates.\n",
    "    Returns:\n",
    "    C (list): the list of candidates.\n",
    "    X (matrix): vectorized representation of the candidates.\n",
    "    \"\"\"\n",
    "    # build the vocabulary, i.e. setting the vector dimensions\n",
    "    dim = set([])\n",
    "    # for k, v in self.candidates.iteritems():\n",
    "    # iterate Python 2/3 compatible\n",
    "     \n",
    "    for (k, v) in candidates.items():\n",
    "        for w in v.lexical_form:\n",
    "            dim.add(w)\n",
    "    dim = list(dim)\n",
    "    # vectorize the candidates Python 2/3 + sort for random issues\n",
    "    C = list(candidates) #.keys()\n",
    "    C.sort()\n",
    "\n",
    "    X = np.zeros((len(C), len(dim)))\n",
    "    for i, k in enumerate(C):\n",
    "        for w in candidates[k].lexical_form:\n",
    "            X[i, dim.index(w)] += 1\n",
    "\n",
    "    return C , X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_clustering(candidates , threshold=0.74, method='average'):\n",
    "    \"\"\"Clustering candidates into topics.\n",
    "        Args:\n",
    "        threshold (float): the minimum similarity for clustering, defaults\n",
    "        to 0.74, i.e. more than 1/4 of stem overlap similarity.\n",
    "        method (str): the linkage method, defaults to average.\n",
    "    \"\"\"\n",
    "    topic = []\n",
    "    # handle document with only one candidate\n",
    "    if len(candidates) == 1:\n",
    "        topics.append([list(candidates)[0]])\n",
    "        return topic\n",
    "\n",
    "\n",
    "    # vectorize the candidates\n",
    "    candidates, X = vectorize_candidates(candidates , topic )\n",
    "\n",
    "    # compute the distance matrix\n",
    "    Y = pdist(X, 'jaccard')\n",
    "    \n",
    "    # compute the clusters\n",
    "    Z = linkage(Y, method=method)\n",
    "    \n",
    "    # form flat clusters\n",
    "    clusters = fcluster(Z, t=threshold, criterion='distance')\n",
    "\n",
    "    # for each topic identifier\n",
    "    for cluster_id in range(1, max(clusters)+1):\n",
    "        topic.append([candidates[j] for j in range(len(clusters))\n",
    "                    if clusters[j] == cluster_id])\n",
    "    \n",
    "    return topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_topic_graph(graph  , topics , candidates ):\n",
    "    \"\"\"Build topic graph.\"\"\"\n",
    "   \n",
    "    # adding the nodes to the graph\n",
    "    graph.add_nodes_from(range(len(topics)))\n",
    "\n",
    "    # loop through the topics to connect the nodes\n",
    "    for i, j in combinations(range(len(topics)), 2):\n",
    "        graph.add_edge(i, j, weight=0.0)\n",
    "        for c_i in topics[i]:\n",
    "            for c_j in topics[j]:\n",
    "                for p_i in candidates[c_i].offsets:\n",
    "                    for p_j in candidates[c_j].offsets:\n",
    "                        gap = abs(p_i - p_j)\n",
    "                        if p_i < p_j:\n",
    "                            gap -= len(candidates[c_i].lexical_form)-1\n",
    "                        if p_j < p_i:\n",
    "                            gap -= len(candidates[c_j].lexical_form)-1\n",
    "                        graph[i][j]['weight'] += 1.0 / gap\n",
    "\n",
    "                                \n",
    "                    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_redundant(candidates, candidate, prev, mininum_length=1):\n",
    "    \"\"\" Test if one candidate is redundant with respect to a list of already\n",
    "    selected candidates. A candidate is considered redundant if it is\n",
    "    included in another candidate that is ranked higher in the list.\n",
    "    Args:\n",
    "    candidate (str): the lexical form of the candidate.\n",
    "    prev (list): the list of already selected candidates (lexicalforms).\n",
    "    mininum_length (int): minimum length (in words) of the candidate\n",
    "    to be considered, defaults to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the tokenized lexical form from the candidate\n",
    "    candidate = candidates[candidate].lexical_form\n",
    "\n",
    "    # only consider candidate greater than one word\n",
    "    if len(candidate) < mininum_length:\n",
    "        return False\n",
    "\n",
    "    # get the tokenized lexical forms from the selected candidates\n",
    "    prev = [candidates[u].lexical_form for u in prev]\n",
    "\n",
    "    # loop through the already selected candidates\n",
    "    for prev_candidate in  prev:\n",
    "        for i in range(len(prev_candidate)-len(candidate)+1):\n",
    "            if candidate == prev_candidate[i:i+len(candidate)]:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _is_alphanum(word, valid_punctuation_marks = '-'):\n",
    "    \"\"\"Check if a word is valid, i.e. it contains only alpha-numeric\n",
    "    characters and valid punctuation marks.\n",
    "    Args:\n",
    "        word (string): a word.\n",
    "        valid_punctuation_marks (str): punctuation marks that are valid\n",
    "        for a candidate, defaults to '-'.\n",
    "    \"\"\"\n",
    "    for punct in valid_punctuation_marks.split():\n",
    "        word = word.replace(punct, '')\n",
    "    return word.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_candidate( candidates ,  words, offset, sentence_id):\n",
    "        \"\"\" Add a keyphrase candidate to the candidates container.\n",
    "            Args:\n",
    "                words (list): the words (surface form) of the candidate.\n",
    "                stems (list): the stemmed words of the candidate.\n",
    "                pos (list): the Part-Of-Speeches of the words in the candidate.\n",
    "                offset (int): the offset of the first word of the candidate.\n",
    "                sentence_id (int): the sentence id of the candidate.\n",
    "        \"\"\"\n",
    "        stemmer='porter'\n",
    "        stems = []\n",
    "        for j, word in enumerate(words):\n",
    "            stems.append(Stemmer(stemmer).stem(str(word)))\n",
    "    \n",
    "        \n",
    "        # build the lexical (canonical) form of the candidate using stems\n",
    "        lexical_form = ' '.join(stems)\n",
    "\n",
    "        # add/update the surface forms\n",
    "        a = []\n",
    "        for j , word in enumerate(words):\n",
    "            a.append(str(word))\n",
    "        candidates[lexical_form].surface_forms.append(a)\n",
    "        # add/update the lexical_form\n",
    "        candidates[lexical_form].lexical_form = stems\n",
    "      \n",
    "        # add/update the POS patterns\n",
    "        pos = []\n",
    "        for j , word in enumerate(words):\n",
    "            pos.append(word.pos_)\n",
    "            \n",
    "        candidates[lexical_form].pos_patterns.append(pos)\n",
    "\n",
    "        # add/update the offsets\n",
    "        candidates[lexical_form].offsets.append(offset)\n",
    "        \n",
    "        # add/update the sentence ids\n",
    "        candidates[lexical_form].sentence_ids.append(sentence_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TopicRank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TopicRank(doc, nkeyterms = 10 , pos = None):\n",
    "    \n",
    "    ##STEP  1 : candidate_selection\n",
    "    if pos is None:\n",
    "        pos_ = set(['NOUN', 'PROPN', 'ADJ'])  \n",
    "     \n",
    "        #### select sequence of adjectives and nouns\n",
    "    \n",
    "    sentences = []  \n",
    "    t = []\n",
    "    k = 0\n",
    "    candidates = defaultdict(Candidate)\n",
    "    # loop through the sentences\n",
    "    for i in doc.sents:\n",
    "        l = 0 \n",
    "        for _ in i :\n",
    "            l = l+1\n",
    "        for j in i : \n",
    "            t.append(str(j))\n",
    "        sentences.append(list(t))\n",
    "        t = []\n",
    "        a = sentences\n",
    "        \n",
    "        # compute the offset shift for the sentence\n",
    "        shift = sum([ len(s) for s in a[0:k]])\n",
    "        k = k + 1\n",
    "        \n",
    "        # container for the sequence (defined as list of offsets)\n",
    "        seq  = [ ]\n",
    "        # loop through the tokens\n",
    "        for j in enumerate(i):\n",
    "        \n",
    "            # add candidate offset in sequence and continue if not last word\n",
    "            \n",
    "            if j[1].pos_ in pos_:\n",
    "                seq.append(j[0])\n",
    "                if j[0] < l:\n",
    "                    continue\n",
    "        \n",
    "            # container for the sequence (defined as list of offsets)\n",
    "            if seq :\n",
    "                        # bias for candidate in last position within sentence\n",
    "                bias = 0                \n",
    "                if j[0] == l :\n",
    "                    bias = 1\n",
    "                words = i[seq[0]:seq[-1]+1]\n",
    "                \n",
    "                \n",
    "    \n",
    "                # add the ngram to the candidate container\n",
    "                add_candidate(candidates , words = i[seq[0]:seq[-1]+1],\n",
    "                                offset= shift + j[0] - len(seq) + bias,\n",
    "                                sentence_id=k)\n",
    "            \n",
    "            # flush sequence container\n",
    "            seq = []      \n",
    "         \n",
    "\n",
    "    \n",
    "    ## STEP 2 : CANDIDATE FILTERING : ( # filter candidates containing stopwords or punctuation marks)\n",
    "    \n",
    "    # initialize stoplist list if not provided\n",
    "    stoplist = list(STOP_WORDS)\n",
    "    \n",
    "    stoplist_ = list(punctuation) + ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-','-rsb-'] + stoplist\n",
    "    mininum_length = 3 \n",
    "    mininum_word_size = 2 \n",
    "    valid_punctuation_marks = '-' \n",
    "    maximum_word_number = 5 \n",
    "    only_alphanum = True \n",
    "    pos_blacklist = []\n",
    "        \n",
    "    # loop throught the candidates    \n",
    "    y = 1\n",
    "    for k in list(candidates):\n",
    "        y = y + 1\n",
    "        # get the candidate\n",
    "        v = candidates[k]\n",
    "        \n",
    "        # get the words from the first occurring surface form\n",
    "        words_lower = [u.lower() for u in v.surface_forms[0]]\n",
    "\n",
    "        \n",
    "        # discard if words are in the stoplist\n",
    "        if set(words).intersection(stoplist_):\n",
    "            del candidates[k]\n",
    "\n",
    "        # discard if tags are in the pos_blacklist\n",
    "        elif set(v.pos_patterns[0]).intersection(pos_blacklist):\n",
    "            del candidates[k]\n",
    "\n",
    "        # discard if containing tokens composed of only punctuation\n",
    "\n",
    "        elif any([set(u).issubset(set(punctuation)) for u in words_lower]):\n",
    "            del candidates[k]\n",
    "\n",
    "        # discard candidates composed of 1-2 characters\n",
    "       \n",
    "        elif len(''.join(words_lower)) < mininum_length:\n",
    "            del candidates[k]\n",
    "\n",
    "        # discard candidates containing small words (1-character)\n",
    "        elif min([len(u) for u in words_lower]) < mininum_word_size:\n",
    "            del candidates[k]\n",
    "\n",
    "        # discard candidates composed of more than 5 words\n",
    "        elif len(v.lexical_form) > maximum_word_number:\n",
    "            del candidates[k]\n",
    "\n",
    "        # discard if not containing only alpha-numeric characters\n",
    "        if only_alphanum and k in candidates:\n",
    "            if not all([_is_alphanum(w) for w in words_lower]):\n",
    "                del self.candidates[k]\n",
    "\n",
    "                \n",
    "    ### STEP 3 : CANDIDATE WEIGHTING             \n",
    "    # cluster the candidates\n",
    "    topic = []\n",
    "    topic = topic_clustering( candidates , threshold = 0.74 , method = 'average')\n",
    "    \n",
    "         \n",
    "    # build the topic graph\n",
    "    graph = nx.Graph()\n",
    "    \"\"\" The topic graph. \"\"\"\n",
    "    graph = build_topic_graph(graph , topic , candidates )\n",
    "\n",
    "   \n",
    "\n",
    "    # compute the word scores using random walk\n",
    "    w = nx.pagerank_scipy(graph, alpha=0.85, weight='weight')\n",
    "    \n",
    "    heuristic = None    \n",
    "    weights = {}\n",
    "    # loop throught the topics\n",
    "    for i, topic_ in enumerate(topic):\n",
    "        # get the offsets of the topic candidates\n",
    "        offsets = [candidates[t].offsets for t in topic_]\n",
    "\n",
    "        # get first candidate from topic   \n",
    "        if heuristic == 'frequent':\n",
    "\n",
    "        # get frequencies for each candidate within the topic\n",
    "            freq = [len(candidates[t].surface_forms) for t in topic_]\n",
    "\n",
    "        # get the indexes of the most frequent candidates\n",
    "            indexes = [j for j, f in enumerate(freq) if f == max(freq)]\n",
    "\n",
    "        # offsets of the indexes\n",
    "            indexes_offsets = [offsets[j] for j in indexes]\n",
    "            most_frequent = indexes_offsets.index(min(indexes_offsets))\n",
    "            weights[topic_[most_frequent]] = w[i]\n",
    "\n",
    "        else:\n",
    "            first = offsets.index(min(offsets))\n",
    "            weights[topic_[first]] = w[i]     \n",
    "            ##### GET_N_GRAMS ######\n",
    "            ### GET_N_BEST_GRAMS   \n",
    "    \n",
    "    ### STEP 4 : GET_N_BEST \n",
    "    n = 10\n",
    "    redundancy_removal = False\n",
    "    stemming = False\n",
    "    \n",
    "    # sort candidates by descending weight\n",
    "    best = sorted(weights, key = weights.get, reverse=True)\n",
    "\n",
    "    # remove redundant candidates\n",
    "    if redundancy_removal:\n",
    "\n",
    "    # initialize a new container for non redundant candidates\n",
    "        non_redundant_best = []\n",
    "\n",
    "        # loop through the best candidates\n",
    "        for candidate in best:\n",
    "\n",
    "        # test wether candidate is redundant\n",
    "            if is_redundant(candidate, non_redundant_best) == True:\n",
    "                continue\n",
    "        # add the candidate otherwise\n",
    "            non_redundant_best.append(candidate)\n",
    "\n",
    "        # break computation if the n-best are found\n",
    "            if len(non_redundant_best) >= n:\n",
    "                break\n",
    "\n",
    "        # copy non redundant candidates in best container\n",
    "        best = non_redundant_best\n",
    "\n",
    "    # get the list of best candidates as (lexical form, weight) tuples\n",
    "    n_best = [( u , weights[u]) for u in best[:min(n, len(best))]]\n",
    "\n",
    "    # replace with surface forms if no stemming\n",
    "    if not stemming:\n",
    "        n_best = [(' '.join(candidates[u].surface_forms[0]).lower(),\n",
    "                    weights[u]) for u in best[:min(n, len(best))]]\n",
    "\n",
    "     # return the list of best candidates\n",
    "    if len(n_best) < n:\n",
    "        logging.warning(\n",
    "                        'Not enough candidates to choose from '\n",
    "                        '({} requested, {} given)'.format(n, len(n_best)))\n",
    "\n",
    "    \n",
    "    \n",
    "    return n_best\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = TopicRank(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blockchain', 0.10728631155393548),\n",
       " ('block chain', 0.07489499535979609),\n",
       " ('which', 0.04264999522178342),\n",
       " ('transaction data', 0.03582978216922917),\n",
       " ('consensus', 0.02865625714472804),\n",
       " ('bitcoin', 0.028261789146602483),\n",
       " ('design', 0.027888904675543134),\n",
       " ('ledger that', 0.027445012181299924),\n",
       " ('cryptocurrency bitcoin', 0.025981430652199065),\n",
       " ('use', 0.02528390393306377)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TOPICS\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
